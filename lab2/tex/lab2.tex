\documentclass{article}

\usepackage[margin=1.2in]{geometry} % page margins
\usepackage[utf8]{inputenc}         % UTF-8 charset fix
\usepackage{mathtools}              % do math
\usepackage{parskip}                % paragraph vertical margin

\renewcommand \thesubsection{\arabic{section}.\Alph{subsection}}

\begin{document}

\section*{Förberedelseuppgifter}

\subsection*{3. Reflektera över vad som är karakteristiskt för normalfördelade
data.}
Tyngdpunkten ligger kring väntevärdet, $\mu$, eller summan av väntevärdena
n*$\mu$.
Summan av en mängd likafördelade s.v:s utfall.

\subsection*{4. Definiera begrepp}
Likelihood: Givet ett utfall, vad är sannolikheten att det var en viss
parameter?\\
Log-likelihood: Likelihood loggad. Underlättar derivering. Bevarar maximum.\\
Samband: Bevarar maximum.\\
MK: Skapar en funktion som minimerar "avståndet" (approximerar flera mätdata)
till mätdata. \\
Maximum-likelihood: Det värde $\theta*$ där L($\theta$) antar sitt största värde
kallas ML-skattningen av $\theta$ \\

\subsection*{5. När X har täthetsfunktionen}
ML-skattningen $b_{obs}^* = \sqrt{\frac{1}{2n}\sum\limits_{i=1}^n x_i^2}$

MK:\\
Om väntevärdet är gemensamt för alla observerade data och väntevärdesfunktionen
är känd, vilket den är eftersom vi känner till fördelningen, så kan man skriva
om MK-formeln enligt def 11.8 på s259. Kontentan är att det är inversen av
väntevärdesfunktionen till vilken tar in det aritmetiska medelvärdet.

Väntevärdesfunktionens invers $b^{-1}(x) = \frac{x}{\sqrt{\pi/2}}$ \\
$b_{obs}^*(\bar{x}) = \frac{\bar{x}}{\sqrt{\pi/2}}$


\subsection*{6. Approximativt konfidensintervall}
Vi kan anta att b är normalfördelad, på grund av att vi antar att vi har många
likafördelade Rayleigh-fördelade s.v:s. $1 - \alpha$ är konfidensgraden, alltså
med vilken sannolikhet det gäller att den ligger i intervallet.

$I_{{\mu }}=({\bar  {x}}-{\frac  {\lambda _{{\alpha /2}}\sigma }{{\sqrt
{n}}}},{\bar  {x}}+{\frac  {\lambda _{{\alpha /2}}\sigma }{{\sqrt  {n}}}})$

\subsection*{7. Linjär regression}
Linjär regression är att en linjär funktion $ax+b$ anpassas till data för att
minimera felet till dessa. Felet kan beräknas med MK t.ex.

Polynomregression är som linjär regression med skillnaden att ett polynom av
passande grad anpassas till datat.


\subsection*{8. Bootstrap}
Bootstrap: Utvidgar en liten mängd spridd mätdata och få en bra approximation av
hur datat hade sett ut ifall man hade haft fler mätvärden. Om man ökar antalet
bootstrapreplikat så blir approximationen bättre.
\\Om man ökar my så förskjuts väntevärdet i x-led.
\\M är antalet indata, så högre M gör att det blir färre "glapp" att fylla i. Så
om M ökar blir resultatet bättre för initiala mätdata. Stort M gör att man inte
behöver göra någon bootstrap.

\subsection*{Problem 1. Maximum likelihood/Minsta kvadrat}
ML = 5.6885
MK = 4.0201
Vi ser att MK skatningen är närmare det faktiska värdet 4 än ML skattningen.

\subsection*{Problem 2.}
my\_est = 1.0205
upper\_bound = 1.0306
lower\_bound = 1.0106

\subsection*{Problem 3.}
Normalfördelningen passar väl in tills x > 980 då grafen avviker ifrån
fördelningen. Pga toppen vid 1000 kommer det bli svårt att hitta en fördelning
som passar.

\subsection*{Problem 4a.}
27.0222

\subsection*{Problem 5}

Bootstrapreplikaten ser ut att följa normalfördelningen.

Kvantilen enligt Matlab: 49.2398  233.8278
Beräknat konfidensintervall enligt känt tillvägagångssätt: 56.3914  232.9574

\end{document}
