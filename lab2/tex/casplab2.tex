\documentclass{article}

\usepackage[margin=1.2in]{geometry} % page margins
\usepackage[utf8]{inputenc}         % UTF-8 charset fix
\usepackage{mathtools}              % do math
\usepackage{parskip}                % paragraph vertical margin

\renewcommand \thesubsection{\arabic{section}.\Alph{subsection}}

\begin{document}

\section*{Förberedelseuppgifter}

\subsection*{3. Reflektera över vad som är karakteristiskt för normalfördelade
data.}
Tyngdpunkten ligger kring väntevärdet, $\mu$, eller summan av väntevärdena
n*$\mu$.
Summan av en mängd likafördelade s.v:s utfall.

\subsection*{4. Definiera begrepp}
Likelihood: Givet ett utfall, vad är sannolikheten att det var en viss
parameter?\\
Log-likelihood: Likelihood loggad. Underlättar derivering. Bevarar maximum.\\
Samband: Bevarar maximum.\\
MK: Skapar en funktion som minimerar "avståndet" (approximerar flera mätdata)
till mätdata. \\
Maximum-likelihood: Det värde $\theta*$ där L($\theta$) antar sitt största värde
kallas ML-skattningen av $\theta$ \\

\subsection*{5. När X har täthetsfunktionen}
ML-skattningen $b_{obs}^* = \sqrt{\frac{1}{2n}\sum\limits_{i=1}^n x_i^2}$

MK:\\
Om väntevärdet är gemensamt för alla observerade data och väntevärdesfunktionen
är känd, vilket den är eftersom vi känner till fördelningen, så kan man skriva
om MK-formeln enligt def 11.8 på s259. Kontentan är att det är inversen av
väntevärdesfunktionen till vilken tar in det aritmetiska medelvärdet.

Väntevärdesfunktionens invers $b^{-1}(x) = \frac{x}{\sqrt{\pi/2}}$ \\
$b_{obs}^*(\bar{x}) = \frac{\bar{x}}{\sqrt{\pi/2}}$


\subsection*{6. Approximativt konfidensintervall}
Vi kan anta att b är normalfördelad, på grund av att vi antar att vi har många
likafördelade Rayleigh-fördelade s.v:s. $1 - \alpha$ är konfidensgraden, alltså
med vilken sannolikhet det gäller att den ligger i intervallet.

$I_{{\mu }}=({\bar  {x}}-{\frac  {\lambda _{{\alpha /2}}\sigma }{{\sqrt
{n}}}},{\bar  {x}}+{\frac  {\lambda _{{\alpha /2}}\sigma }{{\sqrt  {n}}}})$

\subsection*{7. Linjär regression}
Linjär regression är att en linjär funktion $ax+b$ anpassas till data för att
minimera felet till dessa. Felet kan beräknas med MK t.ex.

Polynomregression är som linjär regression med skillnaden att ett polynom av
passande grad anpassas till datat.


\subsection*{9. Bootstrap}
Bootstrap: Utvidgar en liten mängd spridd mätdata och få en bra approximation av
hur datat hade sett ut ifall man hade haft fler mätvärden. Om man ökar antalet
bootstrapreplikat så blir approximationen bättre.
Om man ökar my så förskjuts väntevärdet i x-led.
M är antalet indata, så högre M gör att det blir färre "glapp" att fylla i. Så
om M ökar blir resultatet bättre för initiala mätdata. Stort M gör att man inte
behöver göra någon bootstrap.

\section{Problem}
\subsection*{1. ML/MK}
Skattningarna är oerhört bra. Vid flera körningar är skattningen korrekt på upp
till 2 decimaler. Täthetsfunktionen överrensstämmer med stapeldiagrammet(hist).

\subsection*{2. Konfidensintervall}
$lower bound = 1.0071$
$my est = 1.0205$
$upper bound = 1.0339$\\
Slutsats: Approximerade fördelningen följer datat.


\subsection*{3. Resistorer}
Vi tror att resistorernas motstånd är ungefär normalfördelade men med ett par
resistorer med utstickande resistans (1000). Eftersom vi har så många mätdata
kan vi inte utesluta någon fördelning, eftersom CGS säger att oavsett fördelning
så kommer det gå mot en normalfördelning.


\subsection*{4a. Linjär regression}
2020: 27.0222 \\
Kommentar: Det är konstigt att antalet transistorer per yta och år minskar
kraftigt vissa år (fladder).

\subsection*{4b. Polynomregression}
$x1y1: x^3, x2y2: x^2, x3y3: x^3$

De ser ut att komma från en normalfördelning enligt normplot. \\
Modellen verkar kunna anpassas väl till data.

\subsection*{5. Bootstrap}
Ser normalfördelat ut. Högre M ger bättre normalfördelning och bättre
approximation nedan.\\

quant = 62.0490, 239.0642 \\
I = 56.4846, 232.8642

\end{document}


